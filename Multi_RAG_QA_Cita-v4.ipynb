{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Conversational RAG Retrieval QA Chain with proper citations, like [1][2] with article title, pages and context\n",
    "# (RAG_QA_Cita-3.ipynb) is the best version App, with conversational RAG and good citation style. The others RAG App created before can be ignored.\n",
    "\n",
    "# Multi-Vector Retriever for RAG on Tables, text, and images \n",
    "# from Artikel_3 (https://blog.langchain.dev/semi-structured-multi-modal-rag/)\n",
    "# Cookbook for multi-modal (text + tables + images) RAG (https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb?ref=blog.langchain.dev)\n",
    "# Artikel_4 (https://python.langchain.com/docs/how_to/multimodal_inputs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bbb626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Resource 1. <How to get your RAG application to return sources?> (https://python.langchain.com/docs/how_to/qa_sources/)\n",
    "\n",
    "# 1. use LangSmith\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de69ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment to connect to LangSmith.\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=\"KI_multi-modal-RAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6f5d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Components\n",
    "# 2.1 Select chat model: Google Gemini\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b434e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Select embedding model: HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d66bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Select vector store: Chroma (install and upgrade langchain_chroma)\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62100062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 96 chunks from 26 pages across 1 PDF files.\n"
     ]
    }
   ],
   "source": [
    "# 3. index our documents:\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 3.1 Load PDF files from a folder\n",
    "import os\n",
    "folder_path = \"D:/4-IntoCode/16_LangChain/AgilProjekt_multiModel/AKAP1/\"  # company folder, use / instead of \\\n",
    "all_docs = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(folder_path, file))\n",
    "        pages = loader.load_and_split()\n",
    "        all_docs.extend(pages)\n",
    "\n",
    "# 3.2 Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = splitter.split_documents(all_docs)\n",
    "print(f\"Loaded {len(docs)} chunks from {len(all_docs)} pages across {len(os.listdir(folder_path))} PDF files.\")\n",
    "# Result: \"Loaded 96 chunks from 26 pages across 1 PDF files.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec1db697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Index chunks\n",
    "_ = vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RAG application: reconstruct the Q&A app with sources\n",
    "# Conversational RAG: additional tool-calling features of chat models, a \"back-and-forth\" conversational user experience.\n",
    "# Read Resource 2. <How to get a RAG application to add citations?> (https://python.langchain.com/docs/how_to/qa_citations/)\n",
    "# get a model to cite which parts of the source documents it referenced in generating its response.\n",
    "# Method: Using tool-calling to cite document IDs;\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "742a302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and compile the exact same application with 2 changes:\n",
    "# 1. add a context key of the state to store retrieved documents;\n",
    "# 2. In the generate step, we pluck out the retrieved documents and populate them in the state.\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Define state for application (modified)\n",
    "class State(MessagesState):\n",
    "    context: List[Document] # change 1\n",
    "\n",
    "\n",
    "\n",
    "# load a retriever and construct our prompt:\n",
    "# In the article 2, the author uses WikipediaRetriever, \n",
    "# Combine_Step_1: use our own MultiVectorRetriever from (Multi_Modal_RAG-v2.ipynb)\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "store = InMemoryStore()\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=store,\n",
    "    id_key=\"doc_id\",  # Keep track of original full content\n",
    ")\n",
    "retriever.search_kwargs[\"k\"] = 4  # number of documents to retrieve\n",
    "\n",
    "# 4.1 Define the tool\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Combine_Step_3: Update the Tool to Use Multi-Vector Retrieval and Store Metadata\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = retriever.invoke(query) # change 3\n",
    "\n",
    "    # Rebuild full documents from store using doc_id, change 4\n",
    "    full_docs = []\n",
    "    for doc in retrieved_docs:\n",
    "        doc_id = doc.metadata[\"doc_id\"]\n",
    "        full_text = retriever.docstore.mget([doc_id])[0]\n",
    "        full_docs.append(Document(page_content=full_text, metadata=doc.metadata))\n",
    "    \n",
    "    serialized = \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata}\\nContent: {doc.page_content}\"\n",
    "        for doc in full_docs\n",
    "    )\n",
    "    return serialized, full_docs\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: State):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "# Combine_Step_2: Summarize Text + Tables and Load into MultiVectorRetriever\n",
    "# Use your partition_pdf + summary chain:\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Use unstructured to extract\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=folder_path + file,\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    ")\n",
    "\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Separate into text and table\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "\n",
    "# Multi-vector retriever\n",
    "# Text and Table summaries\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise and essential summary of the table or text. \n",
    "Each summary should not longer than 10 sentences. Please keep it as short as possible. \\\n",
    "Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \") # use Google Gemini instead of OpenAI\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemma-3-27b-it\", temperature=0)    # use \"gemma-3-27b-it\" instead of gemini-2.0-flash or 1.5\n",
    "\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Summarize each\n",
    "text_summaries = summarize_chain.batch([e.text for e in text_elements][:10], {\"max_concurrency\": 1})\n",
    "table_summaries = summarize_chain.batch([e.text for e in table_elements], {\"max_concurrency\": 1})\n",
    "\n",
    "# Add to retriever\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "\n",
    "# Store original full text in memory, summaries in vectorstore\n",
    "# Before adding summaries to the vectorstore, Add Document Title & Page Metadata\n",
    "text_ids = [str(uuid.uuid4()) for _ in text_elements]\n",
    "retriever.vectorstore.add_documents([\n",
    "    Document(\n",
    "    page_content=summary,\n",
    "    metadata={\n",
    "        \"doc_id\": doc_id,\n",
    "        \"source\": file,  # extract from PDF path\n",
    "        \"page\": getattr(element.metadata, \"page_number\", -1)\n",
    "    }\n",
    ")\n",
    "\n",
    "    for summary, doc_id in zip(text_summaries, text_ids)\n",
    "])\n",
    "retriever.docstore.mset(list(zip(text_ids, [e.text for e in text_elements])))\n",
    "\n",
    "# Same for tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in table_elements]\n",
    "retriever.vectorstore.add_documents([\n",
    "    Document(\n",
    "    page_content=summary,\n",
    "    metadata={\n",
    "        \"doc_id\": doc_id,\n",
    "        \"source\": file,  # extract from PDF path\n",
    "        \"page\": getattr(element.metadata, \"page_number\", -1)\n",
    "    }\n",
    ")\n",
    "    for summary, doc_id in zip(table_summaries, table_ids)\n",
    "])\n",
    "retriever.docstore.mset(list(zip(table_ids, [e.text for e in table_elements])))\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt (customize the prompt)\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"\"\"You are an assistant for question-answering tasks. \n",
    "        ONLY Use the following pieces of retrieved context to answer the question. \n",
    "        For each fact, cite its source number like [1][2]. \n",
    "        At the end of your answer, add a list of sources in the format of [1] <source title>, page <page number> and so on.\n",
    "        If you don't know the answer, If unsure, say 'I don't know'.\"\"\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Process and format the answer \n",
    "    result = llm.invoke(prompt)\n",
    "\n",
    "    # Get the content of the AI message\n",
    "    answer = result.content.strip()\n",
    "\n",
    "    # Try to get any custom metadata or sources (if your LLM provides it through a custom return)\n",
    "    sources = result.additional_kwargs.get(\"source_documents\", [])\n",
    "    \n",
    "    # Add formatted citations (with prefered cictation style)\n",
    "    if sources:\n",
    "        answer += \"\\n\\nSources:\"\n",
    "        for i, doc in enumerate(sources, start=1):\n",
    "            source_info = doc.metadata.get('source', 'Unknown document')\n",
    "            page_info = f\", page {doc.metadata['page']}\" if 'page' in doc.metadata else \"\"\n",
    "            answer += f\"\\n[{i}] {source_info}{page_info}\"\n",
    "    \n",
    "    print(\"Answer:\\n\", answer)\n",
    "    \n",
    "    # Run\n",
    "    context = []\n",
    "    for tool_message in tool_messages:                  # change 2\n",
    "        context.extend(tool_message.artifact)\n",
    "    return {\"messages\": [result], \"context\": context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8abb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the application:\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e2c674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b31fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is AKAP1?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (af68762f-c1ed-4a26-ac2c-0ab236004f86)\n",
      " Call ID: af68762f-c1ed-4a26-ac2c-0ab236004f86\n",
      "  Args:\n",
      "    query: AKAP1\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "\n",
      "Answer:\n",
      " AKAP1 is a protein that targets protein kinase A (PKA) to the mitochondria [1].\n",
      " \n",
      " It is a mitochondrial protein and is involved in cAMP signaling [1]. AKAP1 can regulate mitochondrial function by modulating mitochondrial structure and respiration [1].\n",
      " \n",
      " AKAP1 can be used as a therapeutic target for various diseases, including cancer, heart disease, and diabetes [1].\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "AKAP1 is a protein that targets protein kinase A (PKA) to the mitochondria [1].\n",
      " \n",
      " It is a mitochondrial protein and is involved in cAMP signaling [1]. AKAP1 can regulate mitochondrial function by modulating mitochondrial structure and respiration [1].\n",
      " \n",
      " AKAP1 can be used as a therapeutic target for various diseases, including cancer, heart disease, and diabetes [1].\n"
     ]
    }
   ],
   "source": [
    "# Invoking our application, the retrieved Document objects are accessible from the application state.\n",
    "# # about Text\n",
    "input_message = \"What is AKAP1?\" # the answer should be with ToolMessage\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eeb49ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "tell me wabout AKAP variants?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (6c869469-10a4-4bc6-bfa0-e5c5c5ea4058)\n",
      " Call ID: 6c869469-10a4-4bc6-bfa0-e5c5c5ea4058\n",
      "  Args:\n",
      "    query: AKAP variants\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Error: AttributeError(\"'str' object has no attribute 'metadata'\")\n",
      " Please fix your mistakes.\n",
      "Answer:\n",
      " I am unable to provide information about AKAP variants, as the provided context does not contain any details on this topic.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Question 2: about Table\u001b[39;00m\n\u001b[0;32m      2\u001b[0m input_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell me wabout AKAP variants?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# the answer should be with ToolMessage\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_message\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\china\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2461\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2455\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   2456\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   2457\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   2458\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   2459\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   2460\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 2461\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2462\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2463\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2464\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2465\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2467\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   2468\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\china\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\runner.py:153\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    151\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\china\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\china\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 623\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\china\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[11], line 212\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m    210\u001b[0m context \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool_message \u001b[38;5;129;01min\u001b[39;00m tool_messages:                  \u001b[38;5;66;03m# change 2\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43martifact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [result], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: context}\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable",
      "\u001b[0mDuring task with name 'generate' and id '7a50b83e-e1c1-640d-6fa3-8faa6479686b'"
     ]
    }
   ],
   "source": [
    "# Question 2: about Table\n",
    "input_message = \"tell me wabout AKAP variants?\" # the answer should be with ToolMessage\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
