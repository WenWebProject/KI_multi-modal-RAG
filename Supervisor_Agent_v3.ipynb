{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wenwen Aufgaben:\n",
    "# 1). Build a Conversational RAG Retrieval QA Chain with proper citations, like [1][2] with article title, pages and context\n",
    "# (RAG_QA_Cita-3.ipynb) is the conversational QA_App, answer questions based on given PDFs.\n",
    "\n",
    "# 2). Bild a Multi-Vector RAG, which can make summary of text and tables from a PDF\n",
    "# (Multi_Modal_RAG-v2.ipynb) is the Multi_vector_Model, which can make summary of text and tables from a PDF.\n",
    "\n",
    "# 3). Build a Multi-Modal RAG Retrieval QA Chain with proper citations, like [1][2] with article title, pages and context\n",
    "# (Multi_RAG_QA_Cita-v4.ipynb) is the combination with (RAG_QA_Cita-3.ipynb) and (Multi_Modal_RAG-v2.ipynb), so that my App can make dialog with me, based on the text and tables from given PDFs.\n",
    "\n",
    "# 4). In the end, this (Multi_RAG_Agent.ipynb) is the final version of the app, \n",
    "# which can make dialog with me, based on the text and tables from given PDFs, \n",
    "# and also can make a summary of the text and tables from a PDF, with proper citation style.\n",
    "\n",
    "# 5). combine all Agents (Multi_RAG_Agent from Wenwen, Web_Search_Agent and Data_Science_Agent from Hanna) with Supervisor Agent (from Wenwen)\n",
    "# 6). create a Gradio chat interface\n",
    "# 7). create a Huggingface Space for presentation (https://huggingface.co/spaces/hussamalafandi/test_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: build a supervisor_Agent, to control the RAG_Agent from me and Website_Agent & Data_Science_Agent from Hanna\n",
    "# Create supervisor with langgraph-supervisor\n",
    "# https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/#2-create-supervisor-with-langgraph-supervisor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b90a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 774 docs\n",
      "Total documents in ChromaDB: 774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total documents in ChromaDB: 1161\n",
      "INFO:unstructured_inference:Reading PDF for file: D:/4-IntoCode/16_LangChain/AgilProjekt_multiModel/Raw_Data/Apple1/10-K-2022.pdf ...\n"
     ]
    }
   ],
   "source": [
    "# Multi_RAG_Agent_Cleaned.py\n",
    "# Cleaned-up and corrected version of Multi_RAG Agent script by Wenwen\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "import uuid\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "from typing import List, Any\n",
    "from pydantic import BaseModel\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langgraph.graph import MessagesState, StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.tools import tool\n",
    "from langchain.schema.messages import SystemMessage, AIMessage\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ========== 1. Setup Logging and Environment ==========\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter LangSmith API Key: \")\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"KI_multi-modal-RAG\"\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter Google API Key: \")\n",
    "\n",
    "# ========== 2. Initialize Models and Vector Store ==========\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# to Fix: 1. Delete and Rebuild the ChromaDB from Scratch\n",
    "import shutil\n",
    "shutil.rmtree(\"./chroma_db\", ignore_errors=True)\n",
    "# 2. Re-initialize Chroma with persist directory\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "# 3. Add all 4419 documents\n",
    "print(f\"Adding {len(docs)} docs\")\n",
    "vector_store.add_documents(docs)\n",
    "# 4.  Verify\n",
    "print(\"Total documents in ChromaDB:\", len(vector_store.get()['documents']))\n",
    "\n",
    "# ========== 3. Load and Index Documents ==========\n",
    "folder_path = \"D:/4-IntoCode/16_LangChain/AgilProjekt_multiModel/Raw_Data/Apple1/\"\n",
    "all_docs = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(folder_path, file))\n",
    "        pages = loader.load_and_split()\n",
    "        all_docs.extend(pages)\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = splitter.split_documents(all_docs)\n",
    "vector_store.add_documents(docs)\n",
    "logging.info(f\"Total documents in ChromaDB: {len(vector_store.get()['documents'])}\")\n",
    "\n",
    "# ========== 4. Setup MultiVectorRetriever ==========\n",
    "store = InMemoryStore()\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=store,\n",
    "    id_key=\"doc_id\"\n",
    ")\n",
    "retriever.search_kwargs[\"k\"] = 4\n",
    "\n",
    "# ========== 5. Retrieval Tool ==========\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve relevant documents using multi-vector retriever.\"\"\"\n",
    "    try:\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        full_docs = [\n",
    "            Document(page_content=retriever.docstore.mget([doc.metadata[\"doc_id\"]])[0], metadata=doc.metadata)\n",
    "            for doc in retrieved_docs if \"doc_id\" in doc.metadata\n",
    "        ]\n",
    "        serialized = \"\\n\\n\".join(\n",
    "            f\"Source: {doc.metadata.get('source', 'Unknown')}\\nPage: {doc.metadata.get('page', 'N/A')}\\nContent: {doc.page_content}\"\n",
    "            for doc in full_docs\n",
    "        )\n",
    "        return {\"content\": serialized, \"artifact\": full_docs}\n",
    "    except Exception as e:\n",
    "        return {\"content\": f\"Retrieval error: {str(e)}\", \"artifact\": []}\n",
    "\n",
    "# ========== 6. Summarization of PDF Elements ==========\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=os.path.join(folder_path, os.listdir(folder_path)[0]),\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    ")\n",
    "\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "elements = [\n",
    "    Element(type=\"table\" if \"Table\" in str(type(e)) else \"text\", text=str(e))\n",
    "    for e in raw_pdf_elements\n",
    "]\n",
    "\n",
    "text_elements = [e for e in elements if e.type == \"text\"]\n",
    "table_elements = [e for e in elements if e.type == \"table\"]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an assistant tasked with summarizing tables and text. Give a concise and essential summary of the table or text.\n",
    "Each summary should be under 10 sentences. Table or text chunk: {element}\n",
    "\"\"\")\n",
    "\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | llm | StrOutputParser()\n",
    "\n",
    "text_summaries = summarize_chain.batch([e.text for e in text_elements], {\"max_concurrency\": 1})\n",
    "table_summaries = summarize_chain.batch([e.text for e in table_elements], {\"max_concurrency\": 1})\n",
    "\n",
    "text_ids = [str(uuid.uuid4()) for _ in text_elements]\n",
    "retriever.vectorstore.add_documents([\n",
    "    Document(page_content=summary, metadata={\"doc_id\": doc_id, \"element_type\": \"text\"})\n",
    "    for summary, doc_id in zip(text_summaries, text_ids)\n",
    "])\n",
    "retriever.docstore.mset(list(zip(text_ids, [e.text for e in text_elements])))\n",
    "\n",
    "# Repeat for tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in table_elements]\n",
    "retriever.vectorstore.add_documents([\n",
    "    Document(page_content=summary, metadata={\"doc_id\": doc_id, \"element_type\": \"table\"})\n",
    "    for summary, doc_id in zip(table_summaries, table_ids)\n",
    "])\n",
    "retriever.docstore.mset(list(zip(table_ids, [e.text for e in table_elements])))\n",
    "\n",
    "# ========== 7. Multi-RAG Agent ==========\n",
    "def query_or_respond(state: MessagesState):\n",
    "    return {\"messages\": [llm.bind_tools([retrieve]).invoke(state[\"messages\"])]}\n",
    "\n",
    "def generate(state: MessagesState):\n",
    "    tool_messages = [msg for msg in state[\"messages\"] if msg.type == \"tool\"]\n",
    "    if not tool_messages or not tool_messages[0].artifact:\n",
    "        return {\"messages\": [AIMessage(content=\"No relevant information found.\")], \"context\": []}\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(\n",
    "        f\"Document {i+1} (Page {doc.metadata.get('page', 'N/A')}):\\n{doc.page_content}\"\n",
    "        for i, doc in enumerate(tool_messages[0].artifact)\n",
    "    )\n",
    "    system_message = SystemMessage(\n",
    "        content=f\"Use the following context to answer. Cite sources like [1][2].\\n\\n{docs_content}\"\n",
    "    )\n",
    "    conversation = [msg for msg in state[\"messages\"] if msg.type in (\"human\", \"system\")]\n",
    "    answer = llm.invoke([system_message] + conversation).content\n",
    "    return {\"messages\": [AIMessage(content=answer)], \"context\": tool_messages[0].artifact}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(query_or_respond)\n",
    "builder.add_node(ToolNode([retrieve]))\n",
    "builder.add_node(generate)\n",
    "builder.set_entry_point(\"query_or_respond\")\n",
    "builder.add_conditional_edges(\"query_or_respond\", tools_condition, {\"tools\": \"ToolNode\", END: END})\n",
    "builder.add_edge(\"ToolNode\", \"generate\")\n",
    "builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# ========== 8. Run Example ==========\n",
    "question = \"What is the Total net sales in the Year 2020?\"\n",
    "for step in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": question}]}, stream_mode=\"values\"):\n",
    "    step[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Web_Search_Agent.ipynb\" from Hanna\n",
    "#!pip install gradio\n",
    "#!pip install openai requests beautifulsoup4\n",
    "#!pip install langgraph langchain openai gradio beautifulsoup4\n",
    "#!pip install tavily-python\n",
    "#!from tavily import TavilyClient\n",
    "#!pip install langchain tavily-python openai\n",
    "#!pip install python-dotenv\n",
    "from tavily import TavilyClient\n",
    "#!pip install langchain_community\n",
    "#!pip install --upgrade langchain langchain_community\n",
    "\n",
    "from langchain.llms import HuggingFaceHub\n",
    "#!pip install --upgrade huggingface_hub\n",
    "from huggingface_hub import InferenceClient\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import requests\n",
    "import gradio as gr\n",
    "from bs4 import BeautifulSoup\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ API NewsAPI\n",
    "NEWS_API_URL = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "class Config:\n",
    "    @staticmethod\n",
    "    def setup():\n",
    "        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¸Ð· .env Ñ„Ð°Ð¹Ð»Ð°\n",
    "        load_dotenv()\n",
    "        token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "        # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð´Ð»Ñ LangChain, Tavily Ð¸ OpenAI\n",
    "        os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\", \"\")\n",
    "        os.environ[\"LANGCHAIN_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\", \"\")\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\", \"\")\n",
    "        os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\", \"\")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\", \"\")\n",
    "\n",
    "        if os.getenv(\"LANGSMITH_TRACING\", \"false\").lower() == \"true\":\n",
    "            os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÑƒ  # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°\n",
    "\n",
    "client = InferenceClient(token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "\n",
    "# Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
    "response = client.text_generation(\n",
    "    \"Hi! Tell me something interesting..\",\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-base\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
    "\n",
    "print(llm(\"Hi! Tell me something interesting..\"))\n",
    "\n",
    "# ðŸ§  Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Tavily\n",
    "def tavily_search_tool(query: str) -> str:\n",
    "    tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "    results = tavily.search(query=query, search_depth=\"basic\")\n",
    "\n",
    "    summaries = []\n",
    "    for res in results['results'][:3]:\n",
    "        try:\n",
    "            html = requests.get(res['url']).content\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "            summaries.append(text[:500])\n",
    "        except Exception as e:\n",
    "            summaries.append(f\"[Fehler {res['url']}: {e}]\")\n",
    "    return '\\n\\n'.join(summaries)\n",
    "\n",
    "\n",
    "# âœ… Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ LangChain\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Tavily Web Search\",\n",
    "        func=tavily_search_tool,\n",
    "        description=\"\"Searches for up-to-date market information on the Internet at the user's request\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# ðŸ¤– Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð°\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ðŸŽ¯ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°\n",
    "def agent_handler(user_input):\n",
    "    try:\n",
    "        response = agent.run(user_input)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Agent Error: {str(e)}\"\n",
    "\n",
    "# ðŸŽ› Ð˜Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ Gradio\n",
    "gr.Interface(\n",
    "    fn=agent_handler,\n",
    "    inputs=gr.Textbox(label=\"Your query (eg: What news is impacting Google today?))\"),\n",
    "    outputs=gr.Textbox(label=\"Agent's response\"),\n",
    "    title=\"ðŸ§  Web search agent based onTavily + OpenAI\",\n",
    "    description=\"This agent searches for fresh news and information using Tavily and analyzes it using GPT.\"\n",
    ").launch()\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "Web_Search_Agent = create_react_agent(llm, [retrieve])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Data_Science_Agent.ipynb\" from Hanna \n",
    "# Agent analitic plotlib5\n",
    "\n",
    "#!pip install huggingface_hub\n",
    "#!pip install langchain huggingface_hub transformers\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "#!pip install huggingface_hub[hf_xet]\n",
    "#!pip install hf_xet\n",
    "#!pip install --upgrade transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "#!pip install torch\n",
    "#!pip install plotly\n",
    "\n",
    "\n",
    "#!pip install --upgrade transformers langchain huggingface_hub torch\n",
    "\n",
    "\n",
    "#hf_pipeline = pipeline(\"text-generation\", model=\"distilgpt2\", max_new_tokens=100)\n",
    "\n",
    "#!pip install huggingface_hub[hf_xet]\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.tools import BaseTool\n",
    "#from langchain.llms import HuggingFace\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from prophet import Prophet\n",
    "import plotly.graph_objects as go\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÐºÐ»ÑŽÑ‡Ð¸ Ð¸Ð· .env\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Setting up the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# class CSVForecastTool(BaseTool):CSV Ñ Prophet\n",
    "class CSVForecastTool(BaseTool):\n",
    "    name: str = \"CSVForecast\"\n",
    "    description: str = \"Predicts time series from CSV files (columns Date Ð¸ Close)\"\n",
    "    #Add an analysis tool CSV\n",
    "\n",
    "    def _run(self, company_name: str) -> List[Tuple[str, go.Figure]]:\n",
    "     #   company_folder = f\"parsed/{company_name.lower()}/\"\n",
    "        company_folder = f\"parsed/{company_name}/\"\n",
    "\n",
    "        if not os.path.exists(company_folder):\n",
    "            return [(f\"Error: Folder for company{company_name} not found.\", None)]\n",
    "\n",
    "        csv_files = glob.glob(os.path.join(company_folder, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            return [(f\"Error: No CSV files for company{company_name}.\", None)]\n",
    "\n",
    "        results = []\n",
    "        for file in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                if \"Date\" not in df.columns or \"Close\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Data transformation for Prophet\n",
    "                df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "                df = df.dropna(subset=[\"Date\", \"Close\"])\n",
    "                df = df.rename(columns={\"Date\": \"ds\", \"Close\": \"y\"})\n",
    "\n",
    "                # Forecasting with Prophett\n",
    "                model = Prophet()\n",
    "                model.fit(df)\n",
    "                future = model.make_future_dataframe(periods=90)\n",
    "                forecast = model.predict(future)\n",
    "\n",
    "                # ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ°\n",
    "                fig = go.Figure()\n",
    "                fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat'], mode='lines', name='Forecast'))\n",
    "                fig.add_trace(go.Scatter(x=df['ds'], y=df['y'], mode='markers', name='Historical data'))\n",
    "                fig.update_layout(title=f\"Forecast fur {company_name}\", xaxis_title=Datum\", yaxis_title=\"Closing price\")\n",
    "\n",
    "                # Text generation\n",
    "                change = forecast['yhat'].iloc[-1] - forecast['yhat'].iloc[-91]\n",
    "                change_pct = (change / forecast['yhat'].iloc[-91]) * 100\n",
    "                trend = \"Height\" if change > 0 else \"Fall\"\n",
    "                result_text = f\"{company_name}: ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð· Ð½Ð° ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÐºÐ²Ð°Ñ€Ñ‚Ð°Ð»: {trend} ~{abs(change_pct):.2f}%\"\n",
    "                results.append((result_text, fig))\n",
    "\n",
    "            except Exception as e:\n",
    "                results.append((f\"error while processing {file}: {str(e)}\", None))\n",
    "\n",
    "        return results\n",
    "\n",
    "# Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ ÐºÐ°Ð»ÑŒÐºÑƒÐ»ÑÑ‚Ð¾Ñ€Ð°\n",
    "class CalculatorTool(BaseTool):\n",
    "    name: str = \"Calculator\"\n",
    "    description: str = \"Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            return str(eval(query))\n",
    "        except Exception as e:\n",
    "            return f\"ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹: {str(e)}\"\n",
    "\n",
    "# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²\n",
    "csv_forecast_tool = CSVForecastTool()\n",
    "calculator_tool = CalculatorTool()\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"CSVForecast\", func=csv_forecast_tool._run, description=\"ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð· Ð¿Ð¾ CSV Ð´Ð»Ñ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸\"),\n",
    "    Tool(name=\"Calculator\", func=calculator_tool._run, description=\"ÐœÐ°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÐºÐ°Ð»ÑŒÐºÑƒÐ»ÑÑ‚Ð¾Ñ€\"),\n",
    "   # Tool(name=csv_forecast_tool.name, func=csv_forecast_tool._run, description=csv_forecast_tool.description)\n",
    "]\n",
    "\n",
    "# Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ LLM Ñ Hugging Face\n",
    "# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "hf_pipeline = pipeline(\"text-generation\", model=\"gpt2\", max_new_tokens=100)\n",
    "\n",
    "# Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð°\n",
    "output = hf_pipeline(\"Hello, how are you?\")\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "output = hf_pipeline(\"Hello, how are you?\")\n",
    "print(output)\n",
    "#llm_hf = pipeline(\"text-generation\", model=\"gpt2\")                     #HuggingFace(pipeline=hf_pipeline)\n",
    "\n",
    "# ÐžÐ±Ñ‘Ñ€Ñ‚ÐºÐ° Ð² LangChain ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ð¹ llm\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "# Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð°\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,                            #llm_hf,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð²Ñ‹Ð·Ð¾Ð²Ð° Ð°Ð³ÐµÐ½Ñ‚Ð°\n",
    "query = \"ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Nvidia Ð¸ Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¹ Ð³Ñ€Ð°Ñ„Ð¸Ðº\"\n",
    "response = agent.run(query)\n",
    "\n",
    "query = \"ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Nvidia Ð·Ð° 6 Ð¼ÐµÑÑÑ†ÐµÐ², Ð¿Ð¾ÐºÐ°Ð¶Ð¸ Ð³Ñ€Ð°Ñ„Ð¸Ðº Ð¸ ÑÐ¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð¸Ñ€ÑƒÐ¹ Ð½Ð° ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÐºÐ²Ð°Ñ€Ñ‚Ð°Ð».\"\n",
    "response = agent.run(query)\n",
    "\n",
    "# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¸ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ\n",
    "if isinstance(response, list) and isinstance(response[0], tuple):\n",
    "    text, fig = response[0]\n",
    "    print(text)\n",
    "    if fig:\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"ÐžÑ‚Ð²ÐµÑ‚ Ð°Ð³ÐµÐ½Ñ‚Ð°:\", response)\n",
    "\n",
    "#if isinstance(response, list):\n",
    "#    for result in response:\n",
    " #       if isinstance(result, tuple):\n",
    "  #          text, figure = result\n",
    "  #          print(text)\n",
    "   #         if figure:\n",
    "    #            figure.show()  # ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ°\n",
    "    #    else:\n",
    "    #        print(result)  # Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¸Ð»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ\n",
    "#else:\n",
    " #   print(\"ÐžÑ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Ð°Ð³ÐµÐ½Ñ‚Ð° Ð½Ðµ ÑÐ²Ð»ÑÐµÑ‚ÑÑ ÑÐ¿Ð¸ÑÐºÐ¾Ð¼. ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚:\", response)\n",
    "\n",
    "# ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°\n",
    "query = \"Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ð±ÑƒÐ´ÐµÑ‚ 2 + 2?\"\n",
    "response = agent.run(query)\n",
    "print(response)\n",
    "\n",
    "# Ð’Ñ‹Ð²Ð¾Ð´ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°\n",
    "if isinstance(response, list):\n",
    "    for result in response:\n",
    "        if isinstance(result, tuple):\n",
    "            text, figure = result\n",
    "            print(text)\n",
    "            if figure:\n",
    "                figure.show()  # ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ°\n",
    "        else:\n",
    "            print(result)  # Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¸Ð»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ\n",
    "else:\n",
    "    print(\"ÐžÑ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Ð°Ð³ÐµÐ½Ñ‚Ð° Ð½Ðµ ÑÐ²Ð»ÑÐµÑ‚ÑÑ ÑÐ¿Ð¸ÑÐºÐ¾Ð¼. ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚:\", response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "Data_Science_Agent = create_react_agent(llm, [retrieve])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad302683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervisor_Agent from Wenwen\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "supervisor = create_supervisor(\n",
    "    model=init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\"), # use Google Gemini instead of OpenAI\n",
    "    agents=[Multi_RAG_Agent, Web_Search_Agent, Data_Science_Agent],\n",
    "    prompt=(\n",
    "        \"You are a supervisor managing two agents:\\n\"\n",
    "        \"- Multi_RAG_Agent. Assign tasks related to text and table analysis from PDFs to this agent\\n\"\n",
    "        \"- Web_Search_Agent. Assign web search tasks to this agent\\n\"\n",
    "        \"- Data_Science_Agent. Assign data science-related tasks to this agent\\n\"\n",
    "        \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n",
    "        \"Do not do any work yourself.\"\n",
    "    ),\n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode=\"full_history\",\n",
    ").compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Gradio chat interface using a LangChain chat model, from Wenwen\n",
    "import gradio as gr\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "\n",
    "# Initialize the chat model with explicit API key\n",
    "model = supervisor\n",
    "\n",
    "def respond(\n",
    "    message: str,\n",
    "    history: list[list[str]],  # Gradio's history format: [[user_msg, ai_msg], ...]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Respond to user input using the model.\n",
    "    \"\"\"\n",
    "    # Convert Gradio history to LangChain message format\n",
    "    chat_history = []\n",
    "    for human_msg, ai_msg in history:\n",
    "        chat_history.extend([\n",
    "            HumanMessage(content=human_msg),\n",
    "            AIMessage(content=ai_msg)\n",
    "        ])\n",
    "    \n",
    "    # Add the new user message\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "    \n",
    "    # Get the AI's response\n",
    "    response = model.invoke({'messages': chat_history}, config={\"configurable\": {\"thread_id\": \"thread_123\"}})\n",
    "    \n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    # examples=[\"Hello\", \"What's AI?\", \"Tell me a joke\"],\n",
    "    title=\"Gemini Chat\",\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a5a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c484c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
