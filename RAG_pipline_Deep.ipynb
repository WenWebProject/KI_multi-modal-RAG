{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1917550f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Apple Inc.\n",
      "\n",
      "================================================================================\n",
      "SOURCE DOCUMENTS:\n",
      "\n",
      "[Document 1] 10-Q-Q2-2023.pdf (Page 21)\n",
      "--------------------------------------------------\n",
      "financial reporting.\n",
      "Apple Inc. | Q2 2023 Form 10-Q | 19...\n",
      "\n",
      "[Document 2] 10-Q-Q2-2023.pdf (Page 21)\n",
      "--------------------------------------------------\n",
      "financial reporting.\n",
      "Apple Inc. | Q2 2023 Form 10-Q | 19...\n",
      "\n",
      "[Document 3] 10-Q-Q2-2023.pdf (Page 21)\n",
      "--------------------------------------------------\n",
      "financial reporting.\n",
      "Apple Inc. | Q2 2023 Form 10-Q | 19...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Import all necessary packages\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import os\n",
    "\n",
    "# 3. Document Loading with Metadata\n",
    "def load_quarter_specific(folder_path, year=\"2023\", quarter=\"Q2\"):\n",
    "    \"\"\"Load documents for specific quarter with strict filtering\"\"\"\n",
    "    quarter_docs = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\") and f\"{year}\" in file and f\"Q{quarter[-1]}\" in file:\n",
    "            try:\n",
    "                loader = PyPDFLoader(os.path.join(folder_path, file))\n",
    "                pages = loader.load_and_split()\n",
    "                for page in pages:\n",
    "                    page.metadata = {\n",
    "                        \"source\": file,\n",
    "                        \"page\": page.metadata.get(\"page\", \"\"),\n",
    "                        \"year\": year,\n",
    "                        \"quarter\": quarter\n",
    "                    }\n",
    "                quarter_docs.extend(pages)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {str(e)}\")\n",
    "    return quarter_docs\n",
    "\n",
    "# Load documents\n",
    "folder_path = r\"D:\\4-IntoCode\\16_LangChain\\AgilProjekt_multiModel\\Raw_Data\\Apple\"\n",
    "q2_2023_docs = load_quarter_specific(folder_path, \"2023\", \"Q2\")\n",
    "\n",
    "# Split documents\n",
    "# 1. Modify the text splitter for better chunking\n",
    "q2_2023_docs = load_quarter_specific(folder_path, \"2023\", \"Q2\")\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,  # Smaller chunks for precision\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", r\"(?<=\\. )\", \" \"]  # Better sentence preservation\n",
    ")\n",
    "q2_chunks = splitter.split_documents(q2_2023_docs)\n",
    "\n",
    "# 4. Create Vector Store\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=q2_chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_q2_2023\"\n",
    ")\n",
    "\n",
    "# 5. Initialize QA System\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation_side=\"left\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def is_chunk_valid(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    return tokens.shape[1] <= 512\n",
    "\n",
    "# 3. Enhanced document loader with validation\n",
    "def load_valid_chunks(folder_path, target_year=\"2023\"):\n",
    "    valid_chunks = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pdf\") and str(target_year) in file:\n",
    "            loader = PyPDFLoader(os.path.join(folder_path, file))\n",
    "            pages = loader.load_and_split(text_splitter=splitter)\n",
    "            for page in pages:\n",
    "                if is_chunk_valid(page.page_content):\n",
    "                    page.metadata = {\n",
    "                        \"source\": file,\n",
    "                        \"page\": page.metadata.get(\"page\", \"\"),\n",
    "                        \"year\": target_year\n",
    "                    }\n",
    "                    valid_chunks.append(page)\n",
    "                else:\n",
    "                    print(f\"Oversized chunk in {file}, splitting further\")\n",
    "                    # Additional splitting if needed\n",
    "                    sub_chunks = splitter.split_text(page.page_content)\n",
    "                    for i, chunk in enumerate(sub_chunks):\n",
    "                        if is_chunk_valid(chunk):\n",
    "                            new_page = page.copy()\n",
    "                            new_page.page_content = chunk\n",
    "                            new_page.metadata[\"subchunk\"] = i\n",
    "                            valid_chunks.append(new_page)\n",
    "    return valid_chunks\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    "    truncation=True,\n",
    "    no_repeat_ngram_size=2,  # Better than repetition_penalty for FLAN-T5\n",
    ")\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 6. Configure QA Chain with proper prompt\n",
    "# Use simpler 'stuff' chain type for reliability\n",
    "# Define your QA chain with proper filter syntax and MMR retrieval\n",
    "# Enhanced prompt template with strict formatting requirements\n",
    "prompt_template = \"\"\"You are a financial analyst specializing in Apple Inc. Generate a comprehensive Q2 2023 performance report using ONLY the provided context. Follow this exact structure:\n",
    "\n",
    "# Apple Q2 2023 Performance Analysis\n",
    "\n",
    "## Executive Summary\n",
    "[3-4 sentence overview highlighting key performance metrics and trends]\n",
    "\n",
    "## Financial Performance\n",
    "### Revenue\n",
    "- Total Revenue: $X.XB (X% change YoY)\n",
    "  - iPhone: $X.XB (X%)\n",
    "  - Mac: $X.XB (X%)\n",
    "  - Services: $X.XB (X%)\n",
    "  - Other Products: $X.XB (X%)\n",
    "\n",
    "### Profitability\n",
    "- Gross Margin: X.X%\n",
    "- Operating Margin: X.X%\n",
    "- Net Income: $X.XB\n",
    "\n",
    "## Product Highlights\n",
    "[Bullet points of key product announcements/updates]\n",
    "\n",
    "## Market Context\n",
    "[Brief analysis of market conditions and competitive landscape]\n",
    "\n",
    "## Sources\n",
    "[Cite all numbers using [Page X] format]\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"\n",
    "\n",
    "# Create the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": 5,\n",
    "            \"score_threshold\": 0.7,\n",
    "            \"filter\": {\n",
    "                \"$and\": [\n",
    "                    {\"quarter\": \"Q2\"},\n",
    "                    {\"year\": \"2023\"}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PromptTemplate(\n",
    "            template=prompt_template,\n",
    "            input_variables=[\"context\"]\n",
    "        )\n",
    "    },\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Execute with error handling\n",
    "try:\n",
    "    response = qa_chain.invoke({\n",
    "        \"query\": \"Generate detailed Apple Q2 2023 financial analysis\"\n",
    "    })\n",
    "    \n",
    "    # Improved display function\n",
    "    def display_results(response):\n",
    "        print(\"=\"*80)\n",
    "        print(response[\"result\"])\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\nSOURCE DOCUMENTS:\")\n",
    "        for i, doc in enumerate(response[\"source_documents\"][:3]):  # Show top 3 sources\n",
    "            print(f\"\\n[Document {i+1}] {doc.metadata['source']} (Page {doc.metadata.get('page','N/A')})\")\n",
    "            print(\"-\"*50)\n",
    "            print(doc.page_content[:500] + \"...\")\n",
    "    \n",
    "    display_results(response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"Check your vectorstore connection and LLM configuration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
