{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wenwen Aufgaben:\n",
    "# 1). Build a Conversational RAG Retrieval QA Chain with proper citations, like [1][2] with article title, pages and context\n",
    "# (RAG_QA_Cita-3.ipynb) is the conversational QA_App, answer questions based on given PDFs.\n",
    "\n",
    "# 2). Bild a Multi-Vector RAG, which can make summary of text and tables from a PDF\n",
    "# (Multi_Modal_RAG-v2.ipynb) is the Multi_vector_Model, which can make summary of text and tables from a PDF.\n",
    "\n",
    "# 3). Build a Multi-Modal RAG Retrieval QA Chain with proper citations, like [1][2] with article title, pages and context\n",
    "# (Multi_RAG_QA_Cita-v4.ipynb) is the combination with (RAG_QA_Cita-3.ipynb) and (Multi_Modal_RAG-v2.ipynb), so that my App can make dialog with me, based on the text and tables from given PDFs.\n",
    "\n",
    "# 4). In the end, this (Multi_RAG_Agent.ipynb) is the final version of the app, \n",
    "# which can make dialog with me, based on the text and tables from given PDFs, \n",
    "# and also can make a summary of the text and tables from a PDF, with proper citation style.\n",
    "\n",
    "# 5). combine all Agents (Multi_RAG_Agent from Wenwen, Web_Search_Agent and Data_Science_Agent from Hanna) with Supervisor Agent (from Wenwen)\n",
    "# 6). create a Gradio chat interface\n",
    "# 7). create a Huggingface Space for presentation (https://huggingface.co/spaces/hussamalafandi/test_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: build a supervisor_Agent, to control the RAG_Agent from me and Website_Agent from Hanna\n",
    "# Create supervisor with langgraph-supervisor\n",
    "# https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/#2-create-supervisor-with-langgraph-supervisor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Multi_RAG_Agent.ipynb\" from Wenwen\n",
    "# 1. use LangSmith\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Configure environment to connect to LangSmith.\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=\"KI_multi-modal-RAG\"\n",
    "\n",
    "# 2. Components\n",
    "# 2.1 Select chat model: Google Gemini\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "# 2.2 Select embedding model: HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# 2.3 Select vector store: Chroma (install and upgrade langchain_chroma)\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "# 3. index our documents:\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 3.1 Load PDF files from a folder\n",
    "import os\n",
    "folder_path = \"D:/4-IntoCode/16_LangChain/AgilProjekt_multiModel/Raw_Data/Apple/\"  # company folder, use / instead of \\\n",
    "all_docs = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(folder_path, file))\n",
    "        pages = loader.load_and_split()\n",
    "        all_docs.extend(pages)\n",
    "\n",
    "# 3.2 Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = splitter.split_documents(all_docs)\n",
    "print(f\"Loaded {len(docs)} chunks from {len(all_docs)} pages across {len(os.listdir(folder_path))} PDF files.\")\n",
    "# Result: \"Loaded 4419 chunks from 1347 pages across 22 PDF files.\"\n",
    "\n",
    "# 3.3 Index chunks\n",
    "_ = vector_store.add_documents(documents=docs)\n",
    "\n",
    "# Check 1: Are the documents actually in the vectorstore?\n",
    "print(f\"Total documents in ChromaDB: {len(vector_store.get())}\")\n",
    "# Result: \"Total documents in ChromaDB: 7\"\n",
    "\n",
    "print(f\"# of docs to add: {len(docs)}\")  # Should be in thousands, not 7\n",
    "# Result: # of docs to add: 4419\n",
    "'''so your docs list has 4419 chunks to add. ✅ That means:\n",
    "PDF loading ✔️\n",
    "Chunking ✔️\n",
    "Number of expected documents ✔️\n",
    "❌ add_documents() didn't actually store them'''\n",
    "\n",
    "# to Fix: step1. Delete and Rebuild the ChromaDB from Scratch\n",
    "import shutil\n",
    "shutil.rmtree(\"./chroma_db\", ignore_errors=True)\n",
    "\n",
    "# step2. Re-initialize Chroma with persist directory\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# step3. Add all 4419 documents\n",
    "print(f\"Adding {len(docs)} docs\")\n",
    "vector_store.add_documents(docs)\n",
    "\n",
    "# step4.  Verify\n",
    "print(\"Total documents in ChromaDB:\", len(vector_store.get()['documents']))\n",
    "# Should print 4419\n",
    "# Result: Total documents in ChromaDB: 4419\n",
    "\n",
    "# 4. Multi_RAG application: reconstruct the Q&A app with citations\n",
    "# Conversational RAG: additional tool-calling features of chat models to cite document IDs;\n",
    "# Multi-Vector RAG: use multiple vector stores to retrieve text and tables from a PDF\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 4.1 Define state for application (modified)\n",
    "class State(MessagesState):\n",
    "    context: List[Document] # change 1\n",
    "\n",
    "# 4.2 load a retriever and construct our prompt:\n",
    "# Combine_Step_1: use our own MultiVectorRetriever from (Multi_Modal_RAG-v2.ipynb)\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "store = InMemoryStore()\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=store,\n",
    "    id_key=\"doc_id\",  # Keep track of original full content\n",
    ")\n",
    "retriever.search_kwargs[\"k\"] = 4  # number of documents to retrieve\n",
    "\n",
    "# 4.3 Define the tool\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Combine_Step_3: Update the Tool to Use Multi-Vector Retrieval and Store Metadata\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = retriever.invoke(query) # change 3\n",
    "\n",
    "    # Rebuild full documents from store using doc_id, change 4\n",
    "    full_docs = []\n",
    "    for doc in retrieved_docs:\n",
    "        doc_id = doc.metadata[\"doc_id\"]\n",
    "        full_text = retriever.docstore.mget([doc_id])[0]\n",
    "        full_docs.append(Document(page_content=full_text, metadata=doc.metadata))\n",
    "    \n",
    "    serialized = \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata}\\nContent: {doc.page_content}\"\n",
    "        for doc in full_docs\n",
    "    )\n",
    "    return {\n",
    "        \"content\": serialized,\n",
    "        \"artifact\": full_docs\n",
    "    }\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: State):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "# 4.4 Combine_Step_2: Summarize Text + Tables and Load into MultiVectorRetriever\n",
    "# Use your partition_pdf + summary chain:\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Use unstructured to extract\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=folder_path + file,\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    ")\n",
    "\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Separate into text and table\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "\n",
    "# 4.5 Text and Table summaries\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise and essential summary of the table or text. \n",
    "Each summary should not longer than 10 sentences. Please keep it as short as possible. \\\n",
    "Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# 4.6 Summary chain\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \") # use Google Gemini instead of OpenAI\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemma-3-27b-it\", temperature=0)    # use \"gemma-3-27b-it\" instead of gemini-2.0-flash or 1.5\n",
    "\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Summarize each\n",
    "text_summaries = summarize_chain.batch([e.text for e in text_elements], {\"max_concurrency\": 1})\n",
    "table_summaries = summarize_chain.batch([e.text for e in table_elements], {\"max_concurrency\": 1})\n",
    "\n",
    "# 4.7 Add to retriever\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "\n",
    "# Store original full text in memory, summaries in vectorstore\n",
    "# Before adding summaries to the vectorstore, Add Document Title & Page Metadata\n",
    "text_ids = [str(uuid.uuid4()) for _ in text_elements]\n",
    "\n",
    "# Build a list of (element, summary, doc_id, metadata)\n",
    "text_triplets = list(zip(text_elements, text_summaries, text_ids))\n",
    "for (element, summary, doc_id) in text_triplets:\n",
    "    idx = text_elements.index(element)\n",
    "    raw_metadata = raw_pdf_elements[idx].metadata\n",
    "    retriever.vectorstore.add_documents([\n",
    "        Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                \"doc_id\": doc_id,\n",
    "                \"source\": file,\n",
    "                \"page\": getattr(raw_metadata, \"page_number\", -1)\n",
    "            }\n",
    "        )\n",
    "    ])\n",
    "retriever.docstore.mset(list(zip(text_ids, [e.text for e in text_elements])))\n",
    "\n",
    "# Same for tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in table_elements]\n",
    "\n",
    "# Build a list of (element, summary, doc_id, metadata)\n",
    "text_triplets = list(zip(text_elements, text_summaries, text_ids))\n",
    "for (element, summary, doc_id) in text_triplets:\n",
    "    idx = text_elements.index(element)\n",
    "    raw_metadata = raw_pdf_elements[idx].metadata\n",
    "    retriever.vectorstore.add_documents([\n",
    "        Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                \"doc_id\": doc_id,\n",
    "                \"source\": file,\n",
    "                \"page\": getattr(raw_metadata, \"page_number\", -1)\n",
    "            }\n",
    "        )\n",
    "    ])\n",
    "retriever.docstore.mset(list(zip(table_ids, [e.text for e in table_elements])))\n",
    "\n",
    "\n",
    "# 4.8 Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # In case tool_messages is empty or malformed:\n",
    "    if not tool_messages or not hasattr(tool_messages[0], \"artifact\"):\n",
    "        raise ValueError(\"No valid tool messages with artifacts found.\")\n",
    "    # Add logging to validate what's being returned\n",
    "    print(\"Tool Message Artifact:\", tool_messages[0].artifact)\n",
    "\n",
    "    \n",
    "    # Format into prompt (customize the prompt)\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in tool_messages[0].artifact)\n",
    "\n",
    "    system_message_content = (\n",
    "        \"\"\"You are an assistant for question-answering tasks. \n",
    "        ONLY Use the following pieces of retrieved context to answer the question. \n",
    "        For each fact, cite its source number like [1][2]. \n",
    "        At the end of your answer, add a list of sources in the format of [1] <source title>, page <page number> and so on.\n",
    "        If you don't know the answer, If unsure, say 'I don't know'.\"\"\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Process and format the answer \n",
    "    result = llm.invoke(prompt)\n",
    "\n",
    "    # Get the content of the AI message\n",
    "    answer = result.content.strip()\n",
    "\n",
    "    # Try to get any custom metadata or sources (if your LLM provides it through a custom return)\n",
    "    sources = tool_messages[0].artifact\n",
    "    \n",
    "    # Add formatted citations (with prefered cictation style)\n",
    "    if sources:\n",
    "        answer += \"\\n\\nSources:\"\n",
    "        for i, doc in enumerate(sources, start=1):\n",
    "            source_info = doc.metadata.get('source', 'Unknown document')\n",
    "            page_info = f\", page {doc.metadata['page']}\" if 'page' in doc.metadata else \"\"\n",
    "            answer += f\"\\n[{i}] {source_info}{page_info}\"\n",
    "    \n",
    "    print(\"Answer:\\n\", answer)\n",
    "    \n",
    "    # Run\n",
    "    context = []\n",
    "    for tool_message in tool_messages:                  # change 2\n",
    "        context.extend(tool_message.artifact)\n",
    "    return {\"messages\": [result], \"context\": context}\n",
    "\n",
    "# 4.9 compile the application:\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# 4.10 Invoking our application, the retrieved Document objects are accessible from the application state.\n",
    "# # about Text\n",
    "input_message = \"What is iPhone net sales in the year of 2020?\" # the answer should be with ToolMessage\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Question 2: about Table\n",
    "input_message = \"tell me about table, which shows net sales by category for 2022, 2021 and 2020?\" # the answer should be with ToolMessage\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# 5. make a Multi_RAG_Agent (after combining the conversation memory and retriever-multi_vector: text, tables)\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "Multi_RAG_Agent = create_react_agent(llm, [retrieve])\n",
    "\n",
    "# inspect the graph:\n",
    "display(Image(Multi_RAG_Agent.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# give a question that would typically require an iterative sequence of retrieval steps to answer:\n",
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "input_message = (\n",
    "    \"What is the Total net sales in the Year 2020?\\n\\n\"\n",
    "    \"Once you get the answer, look up Net sales by category, \"\n",
    "    \"which products were included and how much of each share was.\"\n",
    ")\n",
    "\n",
    "for event in Multi_RAG_Agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Web_Search_Agent.ipynb\" from Hanna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Data_Science_Agent.ipynb\" from Hanna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad302683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervisor_Agent\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "supervisor = create_supervisor(\n",
    "    model=init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\"), # use Google Gemini instead of OpenAI\n",
    "    agents=[Multi_RAG_Agent, Web_Search_Agent, Data_Science_Agent],\n",
    "    prompt=(\n",
    "        \"You are a supervisor managing two agents:\\n\"\n",
    "        \"- a research agent. Assign research-related tasks to this agent\\n\"\n",
    "        \"- a math agent. Assign math-related tasks to this agent\\n\"\n",
    "        \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n",
    "        \"Do not do any work yourself.\"\n",
    "    ),\n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode=\"full_history\",\n",
    ").compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Gradio chat interface using a LangChain chat model\n",
    "import gradio as gr\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "\n",
    "# Initialize the chat model with explicit API key\n",
    "model = supervisor\n",
    "\n",
    "def respond(\n",
    "    message: str,\n",
    "    history: list[list[str]],  # Gradio's history format: [[user_msg, ai_msg], ...]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Respond to user input using the model.\n",
    "    \"\"\"\n",
    "    # Convert Gradio history to LangChain message format\n",
    "    chat_history = []\n",
    "    for human_msg, ai_msg in history:\n",
    "        chat_history.extend([\n",
    "            HumanMessage(content=human_msg),\n",
    "            AIMessage(content=ai_msg)\n",
    "        ])\n",
    "    \n",
    "    # Add the new user message\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "    \n",
    "    # Get the AI's response\n",
    "    response = model.invoke({'messages': chat_history}, config={\"configurable\": {\"thread_id\": \"thread_123\"}})\n",
    "    \n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    # examples=[\"Hello\", \"What's AI?\", \"Tell me a joke\"],\n",
    "    title=\"Gemini Chat\",\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a5a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c484c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
