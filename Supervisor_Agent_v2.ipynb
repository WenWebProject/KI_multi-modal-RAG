{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wenwen Aufgaben:\n",
    "# 1). Build a Conversational RAG Retrieval QA Chain with proper citations, like [1][2] with article title, pages and context\n",
    "# (RAG_QA_Cita-3.ipynb) is the conversational QA_App, answer questions based on given PDFs.\n",
    "\n",
    "# 2). Bild a Multi-Vector RAG, which can make summary of text and tables from a PDF\n",
    "# (Multi_Modal_RAG-v2.ipynb) is the Multi_vector_Model, which can make summary of text and tables from a PDF.\n",
    "\n",
    "# 3). Build a Multi-Modal RAG Retrieval QA Chain with proper citations, like [1][2] with article title, pages and context\n",
    "# (Multi_RAG_QA_Cita-v4.ipynb) is the combination with (RAG_QA_Cita-3.ipynb) and (Multi_Modal_RAG-v2.ipynb), so that my App can make dialog with me, based on the text and tables from given PDFs.\n",
    "\n",
    "# 4). In the end, this (Multi_RAG_Agent.ipynb) is the final version of the app, \n",
    "# which can make dialog with me, based on the text and tables from given PDFs, \n",
    "# and also can make a summary of the text and tables from a PDF, with proper citation style.\n",
    "\n",
    "# 5). combine all Agents (Multi_RAG_Agent from Wenwen, Web_Search_Agent and Data_Science_Agent from Hanna) with Supervisor Agent (from Wenwen)\n",
    "# 6). create a Gradio chat interface\n",
    "# 7). create a Huggingface Space for presentation (https://huggingface.co/spaces/hussamalafandi/test_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: build a supervisor_Agent, to control the RAG_Agent from me and Website_Agent & Data_Science_Agent from Hanna\n",
    "# Create supervisor with langgraph-supervisor\n",
    "# https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/#2-create-supervisor-with-langgraph-supervisor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b90a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 387 chunks from 114 pages across 1 PDF files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 387 documents\n",
      "INFO:__main__:Sample document: page_content='UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "FORM 10-K\n",
      "(Mark One)\n",
      "☒    ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the fiscal year ended September 24, 2022\n",
      "or\n",
      "☐    TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the transition period from              to             .\n",
      "Commission File Number: 001-36743\n",
      "Apple Inc.\n",
      "(Exact name of Registrant as specified in its charter)\n",
      "California 94-2404110\n",
      "(State or other jurisdiction\n",
      "of incorporation or organization)\n",
      "(I.R.S. Employer Identification No.)\n",
      "One Apple Park Way\n",
      "Cupertino, California 95014\n",
      "(Address of principal executive offices) (Zip Code)\n",
      "(408) 996-1010\n",
      "(Registrant’s telephone number, including area code)\n",
      "Securities registered pursuant to Section 12(b) of the Act:\n",
      "Title of each class\n",
      "Trading \n",
      "symbol(s) Name of each exchange on which registered\n",
      "Common Stock, $0.00001 par value per share AAPL The Nasdaq Stock Market LLC' metadata={'producer': 'Wdesk Fidelity Content Translations Version 006.012.078', 'creator': 'Workiva', 'creationdate': '2022-10-27T16:05:06+00:00', 'moddate': '2022-10-27T16:05:06+00:00', 'title': '10-K 2022, 09.24.2022-2022-10-27-08-59', 'author': 'anonymous', 'source': 'D:/4-IntoCode/16_LangChain/AgilProjekt_multiModel/Raw_Data/Apple1/10-K-2022.pdf', 'total_pages': 80, 'page': 0, 'page_label': '1'}\n",
      "INFO:__main__:Sample metadata: {'producer': 'Wdesk Fidelity Content Translations Version 006.012.078', 'creator': 'Workiva', 'creationdate': '2022-10-27T16:05:06+00:00', 'moddate': '2022-10-27T16:05:06+00:00', 'title': '10-K 2022, 09.24.2022-2022-10-27-08-59', 'author': 'anonymous', 'source': 'D:/4-IntoCode/16_LangChain/AgilProjekt_multiModel/Raw_Data/Apple1/10-K-2022.pdf', 'total_pages': 80, 'page': 0, 'page_label': '1'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in ChromaDB: 7\n",
      "# of docs to add: 387\n",
      "Adding 387 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pikepdf._core:pikepdf C++ to Python logger bridge initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in ChromaDB: 774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unstructured_inference:Reading PDF for file: D:/4-IntoCode/16_LangChain/AgilProjekt_multiModel/Raw_Data/Apple1/10-K-2022.pdf ...\n",
      "INFO:unstructured_inference:Loading the Table agent ...\n",
      "INFO:unstructured_inference:Loading the table structure model ...\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n"
     ]
    }
   ],
   "source": [
    "# \"Multi_RAG_Agent.ipynb\" from Wenwen\n",
    "# 1. use LangSmith\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Configure environment to connect to LangSmith.\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=\"KI_multi-modal-RAG\"\n",
    "\n",
    "# 2. Components\n",
    "# 2.1 Select chat model: Google Gemini\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "# 2.2 Select embedding model: HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# 2.3 Select vector store: Chroma (install and upgrade langchain_chroma)\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "# 3. index our documents:\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 3.1 Load PDF files from a folder\n",
    "import os\n",
    "folder_path = \"D:/4-IntoCode/16_LangChain/AgilProjekt_multiModel/Raw_Data/Apple1/\"  # company folder, use / instead of \\\n",
    "all_docs = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(folder_path, file))\n",
    "        pages = loader.load_and_split()\n",
    "        all_docs.extend(pages)\n",
    "\n",
    "# 3.2 Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = splitter.split_documents(all_docs)\n",
    "print(f\"Loaded {len(docs)} chunks from {len(all_docs)} pages across {len(os.listdir(folder_path))} PDF files.\")\n",
    "# Result: \"Loaded 4419 chunks from 1347 pages across 22 PDF files.\"\n",
    "\n",
    "# 3.3 Index chunks\n",
    "_ = vector_store.add_documents(documents=docs)\n",
    "\n",
    "# Add Logging:\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "# Validate Your Data:\n",
    "# Add this after loading documents\n",
    "logger.info(f\"Loaded {len(docs)} documents\")\n",
    "logger.info(f\"Sample document: {docs[0]}\")\n",
    "logger.info(f\"Sample metadata: {docs[0].metadata}\")\n",
    "\n",
    "# Check 1: Are the documents actually in the vectorstore?\n",
    "print(f\"Total documents in ChromaDB: {len(vector_store.get())}\")\n",
    "# Result: \"Total documents in ChromaDB: 7\"\n",
    "\n",
    "print(f\"# of docs to add: {len(docs)}\")  # Should be in thousands, not 7\n",
    "# Result: # of docs to add: 4419\n",
    "'''so your docs list has 4419 chunks to add. ✅ That means:\n",
    "PDF loading ✔️\n",
    "Chunking ✔️\n",
    "Number of expected documents ✔️\n",
    "❌ add_documents() didn't actually store them'''\n",
    "\n",
    "# to Fix: 1. Delete and Rebuild the ChromaDB from Scratch\n",
    "import shutil\n",
    "shutil.rmtree(\"./chroma_db\", ignore_errors=True)\n",
    "# 2. Re-initialize Chroma with persist directory\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "# 3. Add all 4419 documents\n",
    "print(f\"Adding {len(docs)} docs\")\n",
    "vector_store.add_documents(docs)\n",
    "# 4.  Verify\n",
    "print(\"Total documents in ChromaDB:\", len(vector_store.get()['documents']))\n",
    "# Should print 4419\n",
    "# Total documents in ChromaDB: 4419\n",
    "# Result: Total documents in ChromaDB: 4419\n",
    "\n",
    "# 4. Multi_RAG application: reconstruct the Q&A app with citations\n",
    "# Conversational RAG: additional tool-calling features of chat models to cite document IDs;\n",
    "# Multi-Vector RAG: use multiple vector stores to retrieve text and tables from a PDF\n",
    "\n",
    "from langchain_core.messages import SystemMessage, AIMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 4.1 Define state for application (modified)\n",
    "class State(MessagesState):\n",
    "    context: List[Document] # change 1\n",
    "\n",
    "# 4.2 load a retriever and construct our prompt:\n",
    "# Combine_Step_1: use our own MultiVectorRetriever from (Multi_Modal_RAG-v2.ipynb)\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "store = InMemoryStore()\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=store,\n",
    "    id_key=\"doc_id\",  # Keep track of original full content\n",
    ")\n",
    "retriever.search_kwargs[\"k\"] = 4  # number of documents to retrieve\n",
    "\n",
    "# 4.3 Define the tool\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Combine_Step_3: Update the Tool to Use Multi-Vector Retrieval and Store Metadata\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    try:\n",
    "        retrieved_docs = retriever.invoke(query) # change 3\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"content\": \"No relevant documents found\",\n",
    "                \"artifact\": []\n",
    "            }\n",
    "\n",
    "\n",
    "        # Rebuild full documents from store using doc_id, change 4\n",
    "        full_docs = []\n",
    "        for doc in retrieved_docs:\n",
    "            if \"doc_id\" not in doc.metadata:\n",
    "                continue\n",
    "            doc_id = doc.metadata[\"doc_id\"]\n",
    "            full_text = retriever.docstore.mget([doc_id])[0]\n",
    "            if full_text:\n",
    "                full_docs.append(Document(page_content=full_text, metadata=doc.metadata))\n",
    "\n",
    "        serialized = \"\\n\\n\".join(\n",
    "           f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\"\n",
    "           f\"Page: {doc.metadata.get('page', 'N/A')}\\n\"\n",
    "           f\"Content: {doc.page_content}\"\n",
    "           for doc in full_docs\n",
    "        )\n",
    "        return {\n",
    "            \"content\": serialized,\n",
    "            \"artifact\": full_docs\n",
    "       }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"content\": f\"Retrieval error: {str(e)}\",\n",
    "            \"artifact\": []\n",
    "        }\n",
    "    \n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: State):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "# 4.4 Combine_Step_2: Summarize Text + Tables and Load into MultiVectorRetriever\n",
    "# Use your partition_pdf + summary chain:\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Use unstructured to extract\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=folder_path + file,\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    ")\n",
    "\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Separate into text and table\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "\n",
    "# 4.5 Text and Table summaries\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise and essential summary of the table or text. \n",
    "Each summary should not longer than 10 sentences. Please keep it as short as possible. \\\n",
    "Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# 4.6 Summary chain\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \") # use Google Gemini instead of OpenAI\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemma-3-27b-it\", temperature=0)    # use \"gemma-3-27b-it\" instead of gemini-2.0-flash or 1.5\n",
    "\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Summarize each\n",
    "text_summaries = summarize_chain.batch([e.text for e in text_elements], {\"max_concurrency\": 1})\n",
    "table_summaries = summarize_chain.batch([e.text for e in table_elements], {\"max_concurrency\": 1})\n",
    "\n",
    "# 4.7 Add to retriever\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "\n",
    "# Store original full text in memory, summaries in vectorstore\n",
    "# Before adding summaries to the vectorstore, Add Document Title & Page Metadata\n",
    "text_ids = [str(uuid.uuid4()) for _ in text_elements]\n",
    "\n",
    "# Build a list of (element, summary, doc_id, metadata)\n",
    "text_triplets = list(zip(text_elements, text_summaries, text_ids))\n",
    "for (element, summary, doc_id) in text_triplets:\n",
    "    idx = text_elements.index(element)\n",
    "    raw_metadata = raw_pdf_elements[idx].metadata\n",
    "    retriever.vectorstore.add_documents([\n",
    "        Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                \"doc_id\": doc_id,\n",
    "                \"source\": file,\n",
    "                \"page\": getattr(raw_metadata, \"page_number\", -1)\n",
    "            }\n",
    "        )\n",
    "    ])\n",
    "retriever.docstore.mset(list(zip(text_ids, [e.text for e in text_elements])))\n",
    "\n",
    "# Same for tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in table_elements]\n",
    "\n",
    "# Build a list of (element, summary, doc_id, metadata)\n",
    "text_triplets = list(zip(text_elements, text_summaries, text_ids))\n",
    "for (element, summary, doc_id) in text_triplets:\n",
    "    idx = text_elements.index(element)\n",
    "    raw_metadata = raw_pdf_elements[idx].metadata\n",
    "    retriever.vectorstore.add_documents([\n",
    "        Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                \"doc_id\": doc_id,\n",
    "                \"source\": file,\n",
    "                \"page\": getattr(raw_metadata, \"page_number\", -1),\n",
    "                \"element_type\": \"table\"  # Add this to distinguish tables\n",
    "            }\n",
    "        )\n",
    "    ])\n",
    "retriever.docstore.mset(list(zip(table_ids, [e.text for e in table_elements])))\n",
    "\n",
    "\n",
    "# 4.8 Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    try: \n",
    "        # Get tool messages\n",
    "        tool_messages = [msg for msg in state[\"messages\"] if msg.type == \"tool\"]\n",
    "\n",
    "        if not tool_messages or not tool_messages[0].artifact:\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=\"I couldn't retrieve any relevant information.\")],\n",
    "                \"context\": []\n",
    "            }\n",
    "        # Format context\n",
    "        docs_content = \"\\n\\n\".join(\n",
    "            f\"Document {i+1} (Page {doc.metadata.get('page', 'N/A')}):\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(tool_messages[0].artifact)\n",
    "        )\n",
    "\n",
    "        system_message = SystemMessage(\n",
    "            content=f\"\"\"You are an assistant for question-answering tasks. \n",
    "            Use the following retrieved context to answer the question. \n",
    "            Cite sources like [1][2] and list them at the end.\n",
    "            If unsure, say 'I don't know'.\\n\\n{docs_content}\"\"\"\n",
    "        )\n",
    "\n",
    "        conversation = [\n",
    "            msg for msg in state[\"messages\"] \n",
    "            if msg.type in (\"human\", \"system\") or \n",
    "            (msg.type == \"ai\" and not msg.tool_calls)\n",
    "        ]\n",
    "        \n",
    "        response = llm.invoke([system_message] + conversation)\n",
    "        \n",
    "        # Add citations\n",
    "        answer = response.content\n",
    "        if tool_messages[0].artifact:\n",
    "            answer += \"\\n\\nSources:\"\n",
    "            for i, doc in enumerate(tool_messages[0].artifact, 1):\n",
    "                source = doc.metadata.get('source', 'Unknown document')\n",
    "                page = doc.metadata.get('page', 'N/A')\n",
    "                answer += f\"\\n[{i}] {source}, page {page}\"\n",
    "\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=answer)],\n",
    "            \"context\": tool_messages[0].artifact\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error generating response: {str(e)}\")],\n",
    "            \"context\": []\n",
    "        }\n",
    "\n",
    "\n",
    "# 4.9 compile the application:\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# 4.10 Invoking our application, the retrieved Document objects are accessible from the application state.\n",
    "# # about Text\n",
    "input_message = \"What is iPhone net sales in the year of 2020?\" # the answer should be with ToolMessage\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Question 2: about Table\n",
    "input_message = \"tell me about table, which shows net sales by category for 2022, 2021 and 2020?\" # the answer should be with ToolMessage\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# 5. make a Multi_RAG_Agent (after combining the conversation memory and retriever-multi_vector: text, tables)\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "Multi_RAG_Agent = create_react_agent(llm, [retrieve])\n",
    "\n",
    "# inspect the graph:\n",
    "display(Image(Multi_RAG_Agent.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# give a question that would typically require an iterative sequence of retrieval steps to answer:\n",
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "input_message = (\n",
    "    \"What is the Total net sales in the Year 2020?\\n\\n\"\n",
    "    \"Once you get the answer, look up Net sales by category, \"\n",
    "    \"which products were included and how much of each share was.\"\n",
    ")\n",
    "\n",
    "for event in Multi_RAG_Agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Web_Search_Agent.ipynb\" from Hanna\n",
    "#!pip install gradio\n",
    "#!pip install openai requests beautifulsoup4\n",
    "#!pip install langgraph langchain openai gradio beautifulsoup4\n",
    "#!pip install tavily-python\n",
    "#!from tavily import TavilyClient\n",
    "#!pip install langchain tavily-python openai\n",
    "#!pip install python-dotenv\n",
    "from tavily import TavilyClient\n",
    "#!pip install langchain_community\n",
    "#!pip install --upgrade langchain langchain_community\n",
    "\n",
    "from langchain.llms import HuggingFaceHub\n",
    "#!pip install --upgrade huggingface_hub\n",
    "from huggingface_hub import InferenceClient\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import requests\n",
    "import gradio as gr\n",
    "from bs4 import BeautifulSoup\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Конфигурация API NewsAPI\n",
    "NEWS_API_URL = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "class Config:\n",
    "    @staticmethod\n",
    "    def setup():\n",
    "        # Загружаем переменные из .env файла\n",
    "        load_dotenv()\n",
    "        token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "        # Настройка переменных окружения для LangChain, Tavily и OpenAI\n",
    "        os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\", \"\")\n",
    "        os.environ[\"LANGCHAIN_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\", \"\")\n",
    "        os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\", \"\")\n",
    "        os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\", \"\")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\", \"\")\n",
    "\n",
    "        if os.getenv(\"LANGSMITH_TRACING\", \"false\").lower() == \"true\":\n",
    "            os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# Запускаем настройку  # Инициализация клиента\n",
    "\n",
    "client = InferenceClient(token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "\n",
    "# Вызываем модель\n",
    "response = client.text_generation(\n",
    "    \"Hi! Tell me something interesting..\",\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-base\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
    "\n",
    "print(llm(\"Hi! Tell me something interesting..\"))\n",
    "\n",
    "# 🧠 Инструмент Tavily\n",
    "def tavily_search_tool(query: str) -> str:\n",
    "    tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "    results = tavily.search(query=query, search_depth=\"basic\")\n",
    "\n",
    "    summaries = []\n",
    "    for res in results['results'][:3]:\n",
    "        try:\n",
    "            html = requests.get(res['url']).content\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "            summaries.append(text[:500])\n",
    "        except Exception as e:\n",
    "            summaries.append(f\"[Fehler {res['url']}: {e}]\")\n",
    "    return '\\n\\n'.join(summaries)\n",
    "\n",
    "\n",
    "# ✅ Регистрируем инструмент LangChain\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Tavily Web Search\",\n",
    "        func=tavily_search_tool,\n",
    "        description=\"\"Searches for up-to-date market information on the Internet at the user's request\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 🤖 Инициализация агента\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 🎯 Обработчик запроса\n",
    "def agent_handler(user_input):\n",
    "    try:\n",
    "        response = agent.run(user_input)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"❌ Agent Error: {str(e)}\"\n",
    "\n",
    "# 🎛 Интерфейс Gradio\n",
    "gr.Interface(\n",
    "    fn=agent_handler,\n",
    "    inputs=gr.Textbox(label=\"Your query (eg: What news is impacting Google today?))\"),\n",
    "    outputs=gr.Textbox(label=\"Agent's response\"),\n",
    "    title=\"🧠 Web search agent based onTavily + OpenAI\",\n",
    "    description=\"This agent searches for fresh news and information using Tavily and analyzes it using GPT.\"\n",
    ").launch()\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "Web_Search_Agent = create_react_agent(llm, [retrieve])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Data_Science_Agent.ipynb\" from Hanna \n",
    "# Agent analitic plotlib5\n",
    "\n",
    "#!pip install huggingface_hub\n",
    "#!pip install langchain huggingface_hub transformers\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "#!pip install huggingface_hub[hf_xet]\n",
    "#!pip install hf_xet\n",
    "#!pip install --upgrade transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "#!pip install torch\n",
    "#!pip install plotly\n",
    "\n",
    "\n",
    "#!pip install --upgrade transformers langchain huggingface_hub torch\n",
    "\n",
    "\n",
    "#hf_pipeline = pipeline(\"text-generation\", model=\"distilgpt2\", max_new_tokens=100)\n",
    "\n",
    "#!pip install huggingface_hub[hf_xet]\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.tools import BaseTool\n",
    "#from langchain.llms import HuggingFace\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from prophet import Prophet\n",
    "import plotly.graph_objects as go\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from transformers import pipeline\n",
    "\n",
    "# Загружаем ключи из .env\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Setting up the logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# class CSVForecastTool(BaseTool):CSV с Prophet\n",
    "class CSVForecastTool(BaseTool):\n",
    "    name: str = \"CSVForecast\"\n",
    "    description: str = \"Predicts time series from CSV files (columns Date и Close)\"\n",
    "    #Add an analysis tool CSV\n",
    "\n",
    "    def _run(self, company_name: str) -> List[Tuple[str, go.Figure]]:\n",
    "     #   company_folder = f\"parsed/{company_name.lower()}/\"\n",
    "        company_folder = f\"parsed/{company_name}/\"\n",
    "\n",
    "        if not os.path.exists(company_folder):\n",
    "            return [(f\"Error: Folder for company{company_name} not found.\", None)]\n",
    "\n",
    "        csv_files = glob.glob(os.path.join(company_folder, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            return [(f\"Error: No CSV files for company{company_name}.\", None)]\n",
    "\n",
    "        results = []\n",
    "        for file in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                if \"Date\" not in df.columns or \"Close\" not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Data transformation for Prophet\n",
    "                df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "                df = df.dropna(subset=[\"Date\", \"Close\"])\n",
    "                df = df.rename(columns={\"Date\": \"ds\", \"Close\": \"y\"})\n",
    "\n",
    "                # Forecasting with Prophett\n",
    "                model = Prophet()\n",
    "                model.fit(df)\n",
    "                future = model.make_future_dataframe(periods=90)\n",
    "                forecast = model.predict(future)\n",
    "\n",
    "                # Построение графика\n",
    "                fig = go.Figure()\n",
    "                fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat'], mode='lines', name='Forecast'))\n",
    "                fig.add_trace(go.Scatter(x=df['ds'], y=df['y'], mode='markers', name='Historical data'))\n",
    "                fig.update_layout(title=f\"Forecast fur {company_name}\", xaxis_title=Datum\", yaxis_title=\"Closing price\")\n",
    "\n",
    "                # Text generation\n",
    "                change = forecast['yhat'].iloc[-1] - forecast['yhat'].iloc[-91]\n",
    "                change_pct = (change / forecast['yhat'].iloc[-91]) * 100\n",
    "                trend = \"Height\" if change > 0 else \"Fall\"\n",
    "                result_text = f\"{company_name}: Прогноз на следующий квартал: {trend} ~{abs(change_pct):.2f}%\"\n",
    "                results.append((result_text, fig))\n",
    "\n",
    "            except Exception as e:\n",
    "                results.append((f\"error while processing {file}: {str(e)}\", None))\n",
    "\n",
    "        return results\n",
    "\n",
    "# Инструмент калькулятора\n",
    "class CalculatorTool(BaseTool):\n",
    "    name: str = \"Calculator\"\n",
    "    description: str = \"Выполняет математические вычисления\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            return str(eval(query))\n",
    "        except Exception as e:\n",
    "            return f\"Ошибка вычислений: {str(e)}\"\n",
    "\n",
    "# Создание инструментов\n",
    "csv_forecast_tool = CSVForecastTool()\n",
    "calculator_tool = CalculatorTool()\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"CSVForecast\", func=csv_forecast_tool._run, description=\"Прогноз по CSV для компании\"),\n",
    "    Tool(name=\"Calculator\", func=calculator_tool._run, description=\"Математический калькулятор\"),\n",
    "   # Tool(name=csv_forecast_tool.name, func=csv_forecast_tool._run, description=csv_forecast_tool.description)\n",
    "]\n",
    "\n",
    "# Инициализация LLM с Hugging Face\n",
    "# Загружаем модель и токенизатор\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "hf_pipeline = pipeline(\"text-generation\", model=\"gpt2\", max_new_tokens=100)\n",
    "\n",
    "# Генерация текста\n",
    "output = hf_pipeline(\"Hello, how are you?\")\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "output = hf_pipeline(\"Hello, how are you?\")\n",
    "print(output)\n",
    "#llm_hf = pipeline(\"text-generation\", model=\"gpt2\")                     #HuggingFace(pipeline=hf_pipeline)\n",
    "\n",
    "# Обёртка в LangChain совместимый llm\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "# Инициализация агента\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,                            #llm_hf,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Пример вызова агента\n",
    "query = \"Проанализируй Nvidia и построй график\"\n",
    "response = agent.run(query)\n",
    "\n",
    "query = \"Проанализируй Nvidia за 6 месяцев, покажи график и спрогнозируй на следующий квартал.\"\n",
    "response = agent.run(query)\n",
    "\n",
    "# Проверка и отображение\n",
    "if isinstance(response, list) and isinstance(response[0], tuple):\n",
    "    text, fig = response[0]\n",
    "    print(text)\n",
    "    if fig:\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"Ответ агента:\", response)\n",
    "\n",
    "#if isinstance(response, list):\n",
    "#    for result in response:\n",
    " #       if isinstance(result, tuple):\n",
    "  #          text, figure = result\n",
    "  #          print(text)\n",
    "   #         if figure:\n",
    "    #            figure.show()  # Отображение графика\n",
    "    #    else:\n",
    "    #        print(result)  # Если это просто ошибка или информация\n",
    "#else:\n",
    " #   print(\"Ответ от агента не является списком. Полученный ответ:\", response)\n",
    "\n",
    "# Пример запроса\n",
    "query = \"Сколько будет 2 + 2?\"\n",
    "response = agent.run(query)\n",
    "print(response)\n",
    "\n",
    "# Вывод результата\n",
    "if isinstance(response, list):\n",
    "    for result in response:\n",
    "        if isinstance(result, tuple):\n",
    "            text, figure = result\n",
    "            print(text)\n",
    "            if figure:\n",
    "                figure.show()  # Отображение графика\n",
    "        else:\n",
    "            print(result)  # Если это просто ошибка или информация\n",
    "else:\n",
    "    print(\"Ответ от агента не является списком. Полученный ответ:\", response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "Data_Science_Agent = create_react_agent(llm, [retrieve])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad302683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervisor_Agent from Wenwen\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "supervisor = create_supervisor(\n",
    "    model=init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\"), # use Google Gemini instead of OpenAI\n",
    "    agents=[Multi_RAG_Agent, Web_Search_Agent, Data_Science_Agent],\n",
    "    prompt=(\n",
    "        \"You are a supervisor managing two agents:\\n\"\n",
    "        \"- Multi_RAG_Agent. Assign tasks related to text and table analysis from PDFs to this agent\\n\"\n",
    "        \"- Web_Search_Agent. Assign web search tasks to this agent\\n\"\n",
    "        \"- Data_Science_Agent. Assign data science-related tasks to this agent\\n\"\n",
    "        \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n",
    "        \"Do not do any work yourself.\"\n",
    "    ),\n",
    "    add_handoff_back_messages=True,\n",
    "    output_mode=\"full_history\",\n",
    ").compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Gradio chat interface using a LangChain chat model, from Wenwen\n",
    "import gradio as gr\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "\n",
    "# Initialize the chat model with explicit API key\n",
    "model = supervisor\n",
    "\n",
    "def respond(\n",
    "    message: str,\n",
    "    history: list[list[str]],  # Gradio's history format: [[user_msg, ai_msg], ...]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Respond to user input using the model.\n",
    "    \"\"\"\n",
    "    # Convert Gradio history to LangChain message format\n",
    "    chat_history = []\n",
    "    for human_msg, ai_msg in history:\n",
    "        chat_history.extend([\n",
    "            HumanMessage(content=human_msg),\n",
    "            AIMessage(content=ai_msg)\n",
    "        ])\n",
    "    \n",
    "    # Add the new user message\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "    \n",
    "    # Get the AI's response\n",
    "    response = model.invoke({'messages': chat_history}, config={\"configurable\": {\"thread_id\": \"thread_123\"}})\n",
    "    \n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    # examples=[\"Hello\", \"What's AI?\", \"Tell me a joke\"],\n",
    "    title=\"Gemini Chat\",\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a5a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c484c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
