{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16b817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal RAG agent: PDFs â†’ Answers with Citations\n",
    "# 1. PDFs loaded and chunked\n",
    "# 2. LangChain (for chaining logic)\n",
    "# 3. Vector DB created (Chroma)\n",
    "# 4. SentenceTransformers (for text embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78327530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4419 chunks from 1347 pages across 22 PDF files.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load PDF files from a folder\n",
    "import os\n",
    "folder_path = r\"D:\\4-IntoCode\\16_LangChain\\AgilProjekt_multiModel\\Raw_Data\\Apple\"  # company folder, Use raw string\n",
    "all_docs = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(folder_path, file))\n",
    "        pages = loader.load_and_split()\n",
    "        all_docs.extend(pages)\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = splitter.split_documents(all_docs)\n",
    "print(f\"Loaded {len(docs)} chunks from {len(all_docs)} pages across {len(os.listdir(folder_path))} PDF files.\")\n",
    "# Result: \"Loaded 4419 chunks from 1347 pages across 22 PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77e332f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embeddings & Save in Chroma Vector Store\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Create or load vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"  # Persistence now automatic\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62000505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Retrieval QA Chain with proper citations\n",
    "# Load Model via transformers and Wrap in LangChain\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load model & tokenizer\n",
    "# --- 1. Load Model with CPU ---\n",
    "model_name = \"google/flan-t5-base\"\n",
    "# using CPU only: Lightweight, good for basic Q&A\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    truncation_side=\"left\", # Prefer truncating context, not question\n",
    "    model_max_length=512     # Explicitly set model's max length\n",
    ")  \n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Wrap into a generation pipeline\n",
    "# --- 2. Pipeline with Token Limits ---\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,      # Limit response length\n",
    "    max_length=512,   # Total input+output limit\n",
    "    temperature=0.3,  # # Balance creativity/factuality\n",
    "    truncation=True,  # Explicitly enable\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1   # Reduce repetition\n",
    ") \n",
    "\n",
    "# Wrap pipeline in LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Custom prompt\n",
    "# --- 3. Prompt with Citation Instructions ---\n",
    "prompt_template = \"\"\"Answer the question concisely using ONLY the provided context. \n",
    "For each fact, cite its source number like [1][2]. If unsure, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# --- 4. QA Chain with Shortened Context ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",           # Max marginal relevance for diversity\n",
    "        search_kwargs={\n",
    "            \"k\": 4,                  # Optimal balance of sources\n",
    "            \"score_threshold\": 0.3   # Minimum relevance score\n",
    "        }\n",
    "    ),\n",
    "        chain_type_kwargs={\n",
    "        \"prompt\": PROMPT,\n",
    "        \"verbose\": True              # Helpful for debugging\n",
    "    },\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# --- 5. Query Execution with Error Handling ---\n",
    "try:\n",
    "    query = \"Summarize Apple's 2023 Q2 report, citing sources.\"\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    \n",
    "    # Process and format the answer\n",
    "    answer = result[\"result\"].strip()\n",
    "    sources = result[\"source_documents\"]\n",
    "    \n",
    "    # Add formatted citations\n",
    "    if sources:\n",
    "        answer += \"\\n\\nSources:\"\n",
    "        for i, doc in enumerate(sources, start=1):\n",
    "            source_info = doc.metadata.get('source', 'Unknown document')\n",
    "            page_info = f\", page {doc.metadata['page']}\" if 'page' in doc.metadata else \"\"\n",
    "            answer += f\"\\n[{i}] {source_info}{page_info}\"\n",
    "    \n",
    "    print(\"Answer:\\n\", answer)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating answer: {str(e)}\")\n",
    "    print(\"Please try again with a more specific query.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
